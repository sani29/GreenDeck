# -*- coding: utf-8 -*-
"""newsClassifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GcvKeEnsENYHiBJvLUsQxZXpE0_AYsiC
"""

from google.colab import drive
drive.mount('/content/gdrive')

import pandas as pd
import numpy as np
import json
from sklearn.metrics import confusion_matrix 
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style('whitegrid')
# %matplotlib inline
from sklearn.svm import SVC, LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import SGDClassifier
import xgboost as xgb

# Importing the dataset
df = pd.read_json('/content/gdrive/My Drive/News_Category_Dataset.json', lines=True)

df.to_csv()
df.head()

# Drop unnecessary column
df = df.drop(['authors', 'date', 'link'], axis=1)

# Concatenate to both headline and short_description column
# because it is better to have single bag of words rather than two bag of words for each column
df['news'] = df['headline'] + "." + df['short_description']

# now drop both columns
df = df.drop(['short_description', 'headline'], axis = 1)

df.head()

# inspect the amount of each type of news category
df['category'].value_counts()

# Category Visualization
import matplotlib.pyplot as plt
fig = plt.figure(figsize=(8,6))
df.groupby('category').news.count().plot.bar(ylim=0)
plt.show()

# after concatenation
df['news'][0]

df.info()

# Cleaning the texts
from tqdm import tqdm
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from nltk.stem.porter import PorterStemmer
corpus = []
for i in tqdm(range(0, 124989)):
    desc = re.sub('[^a-zA-Z]', ' ', df['news'][i])
    desc = desc.lower()
    desc = desc.split()
    ps = PorterStemmer()
    desc = [ps.stem(word) for word in desc if not word in set(stopwords.words('english'))]
    desc = ' '.join(desc)
    corpus.append(desc)

# Creating the Bag of Words model
from sklearn.feature_extraction.text import CountVectorizer
cv = CountVectorizer()
X_counts = cv.fit_transform(corpus)
X_counts.shape

# TF-IDF
from sklearn.feature_extraction.text import TfidfTransformer
tfidf_transformer = TfidfTransformer()
X_tfidf = tfidf_transformer.fit_transform(X_counts)
Y = df.iloc[:, 0].values
X_tfidf.shape

# Splitting the dataset into the Training set and Test set
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X_tfidf, Y, test_size = 0.20, random_state = 0)

# Hyperparameter Tuning and fitting the classifier model

from time import time
from operator import itemgetter
from sklearn.model_selection import GridSearchCV
from sklearn.datasets import load_digits

#build a classifier
clf = SGDClassifier()

# Utility function to report best scores
def report(grid_scores, n_top=3):
    ts = sorted(grid_scores, key=itemgetter(1), reverse=True)[:n_top]
    for i, score in enumerate(ts):
        print("Model with rank: {0}".format(i + 1))
        print("Mean validation score: {0:.3f} (std: {1:.3f})".format(
            score.mean_validation_score,
            np.std(score.cv_validation_scores)))
        print("Parameters: {0}\n".format(score.parameters))

# use a full grid over all parameters
param_grid = {"max_iter": [1, 5, 10],
              "alpha": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],
              "penalty": ["none", "l1", "l2"]}

# run grid search
grid_search = GridSearchCV(clf, param_grid=param_grid, cv=5, n_jobs=-1)
start = time()
grid_search.fit(X_test, Y_test)

'''print("GridSearchCV took %.2f seconds for %d candidate parameter settings."
#       % (time() - start, len(grid_search.cv_results_)))
report(grid_search.cv_results_)'''
#scores = grid_search.cv_results_['mean_test_score'].reshape(-1, 3).T
sorted(grid_search.cv_results_.keys())

# Predicting the Test set results
Y_pred = grid_search.predict(X_test)

# result
np.mean(Y_pred == Y_test)

# confusion matrix
conf_mat = confusion_matrix(Y_test, Y_pred)
fig, ax = plt.subplots(figsize=(8,6))
sns.heatmap(conf_mat, annot=True, fmt='d',
            xticklabels=df['category'].values, yticklabels=df['news'].values)
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

#the classification report for each class
from sklearn import metrics
print(metrics.classification_report(Y_test, Y_pred, target_names=df['category'].unique()))

